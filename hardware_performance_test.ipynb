{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Performance Testing\n",
    "\n",
    "Test TinyMPC hardware performance by measuring execution time as a function of max_iter for different bitstreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Add driver path\n",
    "os.chdir(\"/home/xilinx/jupyter_notebooks/zhenyu/tinympc_ip_gen/\")\n",
    "\n",
    "sys.path.append('driver')\n",
    "from tinympc_hw import tinympc_hw\n",
    "\n",
    "# Import dynamics for test problem setup\n",
    "from dynamics import LinearizedQuadcopterDynamics, CrazyflieParams, NoiseModel\n",
    "\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynamics model for test problem\n",
    "params = CrazyflieParams()\n",
    "noise_model = NoiseModel()\n",
    "dynamics = LinearizedQuadcopterDynamics(params, noise_model)\n",
    "\n",
    "# Generate system matrices\n",
    "control_freq = 100.0  # Hz\n",
    "A, B = dynamics.generate_system_matrices(control_freq)\n",
    "Q, R = dynamics.generate_cost_matrices()\n",
    "constraints = dynamics.generate_constraints()\n",
    "\n",
    "# System dimensions (fixed for quadrotor)\n",
    "nx = 12  # State dimension\n",
    "nu = 4   # Control dimension\n",
    "\n",
    "print(f\"Test problem configured:\")\n",
    "print(f\"  State dimension (nx): {nx}\")\n",
    "print(f\"  Control dimension (nu): {nu}\")\n",
    "print(f\"  Note: Prediction horizon (N) will be extracted from each bitstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "def generate_test_data(nx, nu, N):\n",
    "    \"\"\"Generate random test data for MPC problem\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Initial state with some deviation from origin\n",
    "    x0 = np.random.randn(nx) * 0.1\n",
    "    x0[2] = 1.0  # Set altitude to 1m\n",
    "    \n",
    "    # Reference trajectory (hover at origin)\n",
    "    xref = np.zeros((N, nx))\n",
    "    xref[:, 2] = 1.0  # Reference altitude\n",
    "    \n",
    "    # Reference control (hover)\n",
    "    uref = np.zeros((N-1, nu))\n",
    "    \n",
    "    return x0, xref, uref\n",
    "\n",
    "def extract_N_from_bitstream(bitstream_path):\n",
    "    \"\"\"Extract N parameter from bitstream filename\"\"\"\n",
    "    import re\n",
    "    filename = bitstream_path.name if hasattr(bitstream_path, 'name') else str(bitstream_path)\n",
    "    pattern = r'tinympcproj_N(\\d+)_'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract N from {filename}, using default N=5\")\n",
    "        return 5\n",
    "\n",
    "# Note: Test data will be generated per bitstream with correct N\n",
    "print(\"Test data generation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all bitstream files in subdirectories\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def find_all_bitstreams(base_path=\".\"):\n",
    "    \"\"\"Find all .bit files in the project directory and subdirectories\"\"\"\n",
    "    bitstream_files = []\n",
    "    \n",
    "    # Search for .bit files recursively\n",
    "    search_pattern = os.path.join(base_path, \"**\", \"*.bit\")\n",
    "    found_files = glob.glob(search_pattern, recursive=True)\n",
    "    \n",
    "    # Convert to Path objects and filter out any invalid paths\n",
    "    for file_path in found_files:\n",
    "        path_obj = Path(file_path)\n",
    "        if path_obj.exists() and path_obj.is_file():\n",
    "            bitstream_files.append(path_obj)\n",
    "    \n",
    "    # Also check specific known locations\n",
    "    known_dirs = [\"bitstream\", \"impl\", \"output\", \"build\"]\n",
    "    for dir_name in known_dirs:\n",
    "        dir_path = Path(base_path) / dir_name\n",
    "        if dir_path.exists() and dir_path.is_dir():\n",
    "            bit_files = list(dir_path.glob(\"*.bit\"))\n",
    "            for bit_file in bit_files:\n",
    "                if bit_file not in bitstream_files:\n",
    "                    bitstream_files.append(bit_file)\n",
    "    \n",
    "    return sorted(bitstream_files)\n",
    "\n",
    "def test_bitstream_performance(bitstream_path, test_maxiter_values=[10, 100, 1000], num_trials=10):\n",
    "    \"\"\"\n",
    "    Test a bitstream with specific max_iter values, with warmup run\n",
    "    \n",
    "    Args:\n",
    "        bitstream_path: Path to bitstream file\n",
    "        test_maxiter_values: List of max_iter values to test (default: [10, 100, 1000])\n",
    "        num_trials: Number of trials per max_iter value (excluding warmup)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results with average execution times\n",
    "    \"\"\"\n",
    "    # Extract N from bitstream filename\n",
    "    N = extract_N_from_bitstream(bitstream_path)\n",
    "    \n",
    "    # Generate test data with correct dimensions\n",
    "    nx = 12  # State dimension (fixed for quadrotor)\n",
    "    nu = 4   # Control dimension (fixed for quadrotor)\n",
    "    x0_test, xref_test, uref_test = generate_test_data(nx, nu, N)\n",
    "    \n",
    "    # Initialize hardware solver\n",
    "    hw_solver = tinympc_hw(bitstream_path=str(bitstream_path))\n",
    "    \n",
    "    results = {\n",
    "        'bitstream': str(bitstream_path),\n",
    "        'N': N,\n",
    "        'maxiter_values': test_maxiter_values,\n",
    "        'avg_times': {},\n",
    "        'all_times': {}\n",
    "    }\n",
    "    \n",
    "    for max_iter in test_maxiter_values:\n",
    "        # Set check_termination equal to max_iter as requested\n",
    "        check_termination = max_iter\n",
    "        hw_solver.setup(max_iter=max_iter, check_termination=check_termination, verbose=0)\n",
    "        \n",
    "        # Warmup run (first run to prepare hardware)\n",
    "        hw_solver.set_x0(x0_test)\n",
    "        hw_solver.set_x_ref(xref_test)\n",
    "        hw_solver.set_u_ref(uref_test)\n",
    "        hw_solver.solve(timeout=1.0)  # Warmup - don't record this time\n",
    "        \n",
    "        # Actual timing runs\n",
    "        times = []\n",
    "        for trial in range(num_trials):\n",
    "            # Set problem data\n",
    "            hw_solver.set_x0(x0_test)\n",
    "            hw_solver.set_x_ref(xref_test)\n",
    "            hw_solver.set_u_ref(uref_test)\n",
    "            \n",
    "            # Measure execution time\n",
    "            start_time = time.perf_counter()\n",
    "            success = hw_solver.solve(timeout=1.0)\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            if success:\n",
    "                exec_time = (end_time - start_time) * 1000  # Convert to ms\n",
    "                times.append(exec_time)\n",
    "        \n",
    "        if len(times) > 0:\n",
    "            results['avg_times'][max_iter] = np.mean(times)\n",
    "            results['all_times'][max_iter] = times\n",
    "    \n",
    "    # Cleanup\n",
    "    hw_solver.cleanup()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Find all available bitstreams\n",
    "print(\"Searching for bitstream files...\")\n",
    "bitstreams = find_all_bitstreams()\n",
    "\n",
    "if len(bitstreams) == 0:\n",
    "    print(\"No bitstream files found. Looking in current directory and subdirectories...\")\n",
    "    # Try with absolute path\n",
    "    bitstreams = find_all_bitstreams(\"/home/xilinx/jupyter_notebooks/zhenyu/tinympc_ip_gen/\")\n",
    "\n",
    "print(f\"\\nFound {len(bitstreams)} bitstream file(s):\")\n",
    "for idx, bitstream in enumerate(bitstreams):\n",
    "    print(f\"  {idx+1}. {bitstream}\")\n",
    "\n",
    "# Select first bitstream as default if available\n",
    "if len(bitstreams) > 0:\n",
    "    selected_bitstream = bitstreams[0]\n",
    "    print(f\"\\nDefault selected bitstream: {selected_bitstream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all bitstreams with max_iter = 10, 100, 1000\n",
    "all_test_results = []\n",
    "test_maxiter_values = [10, 100, 1000]\n",
    "\n",
    "if len(bitstreams) > 0:\n",
    "    print(f\"\\nTesting {len(bitstreams)} bitstream(s) with max_iter values: {test_maxiter_values}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, bitstream in enumerate(bitstreams):\n",
    "        print(f\"\\n[{idx+1}/{len(bitstreams)}] Testing: {bitstream.name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Run performance test with warmup\n",
    "            results = test_bitstream_performance(\n",
    "                bitstream, \n",
    "                test_maxiter_values=test_maxiter_values,\n",
    "                num_trials=10  # 10 trials after warmup for each max_iter\n",
    "            )\n",
    "            \n",
    "            all_test_results.append(results)\n",
    "            \n",
    "            # Display results immediately\n",
    "            print(f\"  N parameter: {results['N']}\")\n",
    "            print(f\"  Average execution times (ms):\")\n",
    "            for max_iter in test_maxiter_values:\n",
    "                if max_iter in results['avg_times']:\n",
    "                    avg_time = results['avg_times'][max_iter]\n",
    "                    std_time = np.std(results['all_times'][max_iter])\n",
    "                    print(f\"    max_iter={max_iter:4d}: {avg_time:8.3f} ± {std_time:.3f} ms\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: Failed to test bitstream - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"No bitstreams found to test.\")\n",
    "\n",
    "# Create summary table of all results\n",
    "if len(all_test_results) > 0:\n",
    "    # Build DataFrame for summary\n",
    "    summary_data = []\n",
    "    for result in all_test_results:\n",
    "        row = {\n",
    "            'Bitstream': Path(result['bitstream']).name,\n",
    "            'N': result['N'],\n",
    "        }\n",
    "        # Add average times for each max_iter value\n",
    "        for max_iter in test_maxiter_values:\n",
    "            if max_iter in result['avg_times']:\n",
    "                row[f'max_iter={max_iter} (ms)'] = f\"{result['avg_times'][max_iter]:.3f}\"\n",
    "            else:\n",
    "                row[f'max_iter={max_iter} (ms)'] = \"N/A\"\n",
    "        summary_data.append(row)\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nSUMMARY TABLE - Average Execution Times\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Export to CSV\n",
    "    csv_filename = 'bitstream_performance_summary.csv'\n",
    "    df_summary.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\nResults exported to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Visualize linear relationships\nif len(linear_results) > 0:\n    # Create comprehensive visualization for each bitstream\n    for result in linear_results:\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        fig.suptitle(f'Linear Analysis: {result[\"bitstream\"]} (N={result[\"N\"]})', \n                     fontsize=16, fontweight='bold')\n        \n        # Plot 1: Linear fit with data points\n        ax1 = axes[0, 0]\n        x = np.array(result['max_iter_values'])\n        y = np.array(result['mean_times'])\n        yerr = np.array(result['std_times'])\n        \n        # Plot data points with error bars\n        ax1.errorbar(x, y, yerr=yerr, fmt='o', capsize=5, markersize=8, \n                    label='Measured', color='blue', alpha=0.7)\n        \n        # Plot fitted line\n        x_fit = np.linspace(0, max(x) * 1.1, 100)\n        y_fit = result['slope'] * x_fit + result['intercept']\n        ax1.plot(x_fit, y_fit, 'r--', linewidth=2, \n                label=f'Fit: y = {result[\"slope\"]:.4f}x + {result[\"intercept\"]:.2f}')\n        \n        # Add equation and R² to plot\n        equation_text = f'time = {result[\"slope\"]:.4f} × max_iter + {result[\"intercept\"]:.2f}\\n'\n        equation_text += f'R² = {result[\"r_squared\"]:.6f}'\n        ax1.text(0.05, 0.95, equation_text, transform=ax1.transAxes, \n                fontsize=11, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n        \n        ax1.set_xlabel('max_iter', fontsize=12)\n        ax1.set_ylabel('Execution Time (ms)', fontsize=12)\n        ax1.set_title('Linear Regression Fit', fontsize=14, fontweight='bold')\n        ax1.grid(True, alpha=0.3)\n        ax1.legend(fontsize=10, loc='lower right')\n        \n        # Plot 2: Residuals\n        ax2 = axes[0, 1]\n        residuals = y - (result['slope'] * x + result['intercept'])\n        ax2.scatter(x, residuals, s=50, alpha=0.7, color='green')\n        ax2.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n        ax2.set_xlabel('max_iter', fontsize=12)\n        ax2.set_ylabel('Residual (ms)', fontsize=12)\n        ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')\n        ax2.grid(True, alpha=0.3)\n        \n        # Add residual statistics\n        residual_text = f'Mean: {np.mean(residuals):.4f} ms\\n'\n        residual_text += f'Std: {np.std(residuals):.4f} ms'\n        ax2.text(0.05, 0.95, residual_text, transform=ax2.transAxes,\n                fontsize=10, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n        \n        # Plot 3: Distribution of measurements\n        ax3 = axes[1, 0]\n        positions = result['max_iter_values'][:5]  # Show first 5 for clarity\n        data_to_plot = result['all_measurements'][:5]\n        bp = ax3.boxplot(data_to_plot, positions=positions, widths=5,\n                         patch_artist=True, showmeans=True)\n        for patch in bp['boxes']:\n            patch.set_facecolor('lightblue')\n        ax3.set_xlabel('max_iter', fontsize=12)\n        ax3.set_ylabel('Execution Time (ms)', fontsize=12)\n        ax3.set_title('Measurement Distribution (First 5 Points)', fontsize=14, fontweight='bold')\n        ax3.grid(True, alpha=0.3, axis='y')\n        \n        # Plot 4: Summary table\n        ax4 = axes[1, 1]\n        ax4.axis('off')\n        \n        # Create summary table\n        table_data = [\n            ['Parameter', 'Value', 'Unit'],\n            ['Slope (a)', f'{result[\"slope\"]:.6f}', 'ms/iter'],\n            ['Intercept (b)', f'{result[\"intercept\"]:.3f}', 'ms'],\n            ['Hardware Startup', f'{result[\"intercept\"]:.3f}', 'ms'],\n            ['Per-iteration Cost', f'{result[\"slope\"]:.6f}', 'ms'],\n            ['R-squared', f'{result[\"r_squared\"]:.6f}', '-'],\n            ['P-value', f'{result[\"p_value\"]:.2e}', '-'],\n            ['Data Points', f'{len(result[\"max_iter_values\"])}', '-'],\n            ['Max iter tested', f'{max(result[\"max_iter_values\"])}', '-']\n        ]\n        \n        table = ax4.table(cellText=table_data,\n                         colWidths=[0.4, 0.3, 0.3],\n                         cellLoc='left',\n                         loc='center')\n        table.auto_set_font_size(False)\n        table.set_fontsize(10)\n        table.scale(1.2, 1.8)\n        \n        # Style the header row\n        for i in range(3):\n            table[(0, i)].set_facecolor('#40466e')\n            table[(0, i)].set_text_props(weight='bold', color='white')\n        \n        # Highlight hardware startup time row\n        table[(3, 0)].set_facecolor('#ffe6e6')\n        table[(3, 1)].set_facecolor('#ffe6e6')\n        table[(3, 2)].set_facecolor('#ffe6e6')\n        \n        ax4.set_title('Linear Model Summary', fontsize=14, fontweight='bold', pad=20)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Save figure\n        fig_filename = f'linear_analysis_{result[\"bitstream\"].replace(\".bit\", \"\")}.png'\n        fig.savefig(fig_filename, dpi=150, bbox_inches='tight')\n        print(f\"Figure saved as: {fig_filename}\")\n    \n    # Summary comparison if multiple bitstreams\n    if len(linear_results) > 1:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"COMPARISON ACROSS ALL BITSTREAMS\")\n        print(\"=\" * 80)\n        \n        comparison_data = []\n        for result in linear_results:\n            comparison_data.append({\n                'Bitstream': result['bitstream'],\n                'N': result['N'],\n                'Slope (ms/iter)': f\"{result['slope']:.6f}\",\n                'Intercept/Startup (ms)': f\"{result['intercept']:.3f}\",\n                'R²': f\"{result['r_squared']:.6f}\"\n            })\n        \n        df_comparison = pd.DataFrame(comparison_data)\n        print(df_comparison.to_string(index=False))\n        \n        # Save comparison to CSV\n        csv_filename = 'linear_analysis_comparison.csv'\n        df_comparison.to_csv(csv_filename, index=False)\n        print(f\"\\nComparison saved to: {csv_filename}\")\n        print(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def analyze_linear_relationship(bitstream_path, max_iter_range=None, num_trials=10):\n    \"\"\"\n    Analyze linear relationship between max_iter and execution time\n    \n    Args:\n        bitstream_path: Path to bitstream file\n        max_iter_range: Range of max_iter values to test (default: 10 to 200 in steps of 10)\n        num_trials: Number of trials per max_iter value\n    \n    Returns:\n        dict: Linear regression results and statistics\n    \"\"\"\n    if max_iter_range is None:\n        max_iter_range = list(range(10, 210, 10))  # 10, 20, 30, ..., 200\n    \n    print(f\"Testing linear relationship for: {bitstream_path.name}\")\n    print(f\"Max_iter range: {min(max_iter_range)} to {max(max_iter_range)}\")\n    print(\"-\" * 60)\n    \n    # Extract N from bitstream\n    N = extract_N_from_bitstream(bitstream_path)\n    \n    # Generate test data\n    nx = 12\n    nu = 4\n    x0_test, xref_test, uref_test = generate_test_data(nx, nu, N)\n    \n    # Initialize hardware solver\n    hw_solver = tinympc_hw(bitstream_path=str(bitstream_path))\n    \n    # Collect data points\n    max_iter_values = []\n    mean_times = []\n    std_times = []\n    all_measurements = []\n    \n    for max_iter in max_iter_range:\n        hw_solver.setup(max_iter=max_iter, check_termination=max_iter, verbose=0)\n        \n        # Warmup run\n        hw_solver.set_x0(x0_test)\n        hw_solver.set_x_ref(xref_test)\n        hw_solver.set_u_ref(uref_test)\n        hw_solver.solve(timeout=1.0)\n        \n        # Collect timing data\n        times = []\n        for _ in range(num_trials):\n            hw_solver.set_x0(x0_test)\n            hw_solver.set_x_ref(xref_test)\n            hw_solver.set_u_ref(uref_test)\n            \n            start_time = time.perf_counter()\n            success = hw_solver.solve(timeout=1.0)\n            end_time = time.perf_counter()\n            \n            if success:\n                exec_time = (end_time - start_time) * 1000  # ms\n                times.append(exec_time)\n        \n        if len(times) > 0:\n            max_iter_values.append(max_iter)\n            mean_times.append(np.mean(times))\n            std_times.append(np.std(times))\n            all_measurements.append(times)\n            print(f\"  max_iter={max_iter:3d}: {np.mean(times):.3f} ± {np.std(times):.3f} ms\")\n    \n    # Cleanup\n    hw_solver.cleanup()\n    \n    # Perform linear regression\n    if len(max_iter_values) > 1:\n        slope, intercept, r_value, p_value, std_err = stats.linregress(max_iter_values, mean_times)\n        \n        # Calculate confidence intervals (95%)\n        n = len(max_iter_values)\n        t_val = stats.t.ppf(0.975, n-2)  # 95% confidence\n        \n        # Standard error of slope and intercept\n        x_mean = np.mean(max_iter_values)\n        ss_x = np.sum((np.array(max_iter_values) - x_mean)**2)\n        se_slope = std_err\n        se_intercept = std_err * np.sqrt(np.sum(np.array(max_iter_values)**2) / (n * ss_x))\n        \n        # Confidence intervals\n        slope_ci = (slope - t_val * se_slope, slope + t_val * se_slope)\n        intercept_ci = (intercept - t_val * se_intercept, intercept + t_val * se_intercept)\n        \n        results = {\n            'bitstream': bitstream_path.name,\n            'N': N,\n            'max_iter_values': max_iter_values,\n            'mean_times': mean_times,\n            'std_times': std_times,\n            'all_measurements': all_measurements,\n            'slope': slope,  # a in ax+b\n            'intercept': intercept,  # b in ax+b\n            'r_squared': r_value**2,\n            'p_value': p_value,\n            'std_err': std_err,\n            'slope_ci': slope_ci,\n            'intercept_ci': intercept_ci\n        }\n        \n        print(\"\\n\" + \"=\" * 60)\n        print(\"LINEAR REGRESSION RESULTS\")\n        print(\"=\" * 60)\n        print(f\"Model: time(ms) = {slope:.6f} × max_iter + {intercept:.3f}\")\n        print(f\"\\nCoefficients:\")\n        print(f\"  a (slope): {slope:.6f} ms/iteration\")\n        print(f\"    95% CI: [{slope_ci[0]:.6f}, {slope_ci[1]:.6f}]\")\n        print(f\"  b (intercept): {intercept:.3f} ms (hardware startup time)\")\n        print(f\"    95% CI: [{intercept_ci[0]:.3f}, {intercept_ci[1]:.3f}]\")\n        print(f\"\\nStatistics:\")\n        print(f\"  R-squared: {r_value**2:.6f}\")\n        print(f\"  P-value: {p_value:.2e}\")\n        print(f\"  Standard error: {std_err:.6f}\")\n        print(\"=\" * 60)\n        \n        return results\n    else:\n        print(\"Insufficient data for linear regression\")\n        return None\n\n# Perform linear analysis for each bitstream\nlinear_results = []\n\nif len(bitstreams) > 0:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ANALYZING LINEAR RELATIONSHIP FOR ALL BITSTREAMS\")\n    print(\"=\" * 80)\n    \n    for idx, bitstream in enumerate(bitstreams):\n        print(f\"\\n[{idx+1}/{len(bitstreams)}] Analyzing: {bitstream.name}\")\n        \n        try:\n            result = analyze_linear_relationship(\n                bitstream,\n                max_iter_range=list(range(10, 210, 10)),  # Test from 10 to 200\n                num_trials=10\n            )\n            if result:\n                linear_results.append(result)\n        except Exception as e:\n            print(f\"  ERROR: Failed to analyze - {e}\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"ANALYSIS COMPLETE\")\n    print(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Linear Relationship Analysis: max_iter vs Execution Time\n\nExplore the linear relationship between max_iter and execution time using the model: **time = a × max_iter + b**\n- **a**: Time per iteration (ms/iteration)\n- **b**: Hardware startup/overhead time (ms)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitstream Switch Time Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "all_bitstreams = list(Path(\"bitstream\").glob(\"*.bit\"))\n",
    "\n",
    "# Test bitstream switch time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BITSTREAM SWITCH TIME TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(all_bitstreams) < 2:\n",
    "    print(\"Need at least 2 bitstreams to test switching time\")\n",
    "else:\n",
    "    # Initialize results storage\n",
    "    switch_times = []\n",
    "    \n",
    "    # Number of switches to test\n",
    "    num_tests = 10\n",
    "    \n",
    "    print(f\"Testing {num_tests} switches between bitstreams...\")\n",
    "    \n",
    "    # Alternate between first two bitstreams\n",
    "    for i in range(num_tests):\n",
    "        bitstream = all_bitstreams[i % len(all_bitstreams)]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        new_solver = tinympc_hw(bitstream_path=str(bitstream))\n",
    "        switch_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        switch_times.append(switch_time)\n",
    "        print(f\"Switch {i+1}: {switch_time:.2f} ms\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_switch = np.mean(switch_times)\n",
    "    std_switch = np.std(switch_times)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Average switch time: {avg_switch:.2f} ms\")\n",
    "    print(f\"Standard deviation: {std_switch:.2f} ms\")\n",
    "    print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}