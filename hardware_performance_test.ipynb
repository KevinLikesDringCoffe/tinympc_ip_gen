{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Performance Testing\n",
    "\n",
    "Test TinyMPC hardware performance by measuring execution time as a function of max_iter for different bitstreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Add driver path\n",
    "os.chdir(\"/home/xilinx/jupyter_notebooks/zhenyu/tinympc_ip_gen/\")\n",
    "\n",
    "sys.path.append('driver')\n",
    "from tinympc_hw import tinympc_hw\n",
    "\n",
    "# Import dynamics for test problem setup\n",
    "from dynamics import LinearizedQuadcopterDynamics, CrazyflieParams, NoiseModel\n",
    "\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynamics model for test problem\n",
    "params = CrazyflieParams()\n",
    "noise_model = NoiseModel()\n",
    "dynamics = LinearizedQuadcopterDynamics(params, noise_model)\n",
    "\n",
    "# Generate system matrices\n",
    "control_freq = 100.0  # Hz\n",
    "A, B = dynamics.generate_system_matrices(control_freq)\n",
    "Q, R = dynamics.generate_cost_matrices()\n",
    "constraints = dynamics.generate_constraints()\n",
    "\n",
    "# System dimensions (fixed for quadrotor)\n",
    "nx = 12  # State dimension\n",
    "nu = 4   # Control dimension\n",
    "\n",
    "print(f\"Test problem configured:\")\n",
    "print(f\"  State dimension (nx): {nx}\")\n",
    "print(f\"  Control dimension (nu): {nu}\")\n",
    "print(f\"  Note: Prediction horizon (N) will be extracted from each bitstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "def generate_test_data(nx, nu, N):\n",
    "    \"\"\"Generate random test data for MPC problem\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Initial state with some deviation from origin\n",
    "    x0 = np.random.randn(nx) * 0.1\n",
    "    x0[2] = 1.0  # Set altitude to 1m\n",
    "    \n",
    "    # Reference trajectory (hover at origin)\n",
    "    xref = np.zeros((N, nx))\n",
    "    xref[:, 2] = 1.0  # Reference altitude\n",
    "    \n",
    "    # Reference control (hover)\n",
    "    uref = np.zeros((N-1, nu))\n",
    "    \n",
    "    return x0, xref, uref\n",
    "\n",
    "def extract_N_from_bitstream(bitstream_path):\n",
    "    \"\"\"Extract N parameter from bitstream filename\"\"\"\n",
    "    import re\n",
    "    filename = bitstream_path.name if hasattr(bitstream_path, 'name') else str(bitstream_path)\n",
    "    pattern = r'tinympcproj_N(\\d+)_'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract N from {filename}, using default N=5\")\n",
    "        return 5\n",
    "\n",
    "# Note: Test data will be generated per bitstream with correct N\n",
    "print(\"Test data generation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance_vs_maxiter(bitstream_path, maxiter_values, num_trials=10):\n",
    "    \"\"\"\n",
    "    Test hardware performance for different max_iter values\n",
    "    \n",
    "    Args:\n",
    "        bitstream_path: Path to bitstream file\n",
    "        maxiter_values: List of max_iter values to test\n",
    "        num_trials: Number of trials per max_iter value\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results containing execution times and statistics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'maxiter': [],\n",
    "        'mean_time': [],\n",
    "        'std_time': [],\n",
    "        'min_time': [],\n",
    "        'max_time': [],\n",
    "        'all_times': []\n",
    "    }\n",
    "    \n",
    "    # Extract N from bitstream filename\n",
    "    N = extract_N_from_bitstream(bitstream_path)\n",
    "    print(f\"Extracted N={N} from bitstream filename\")\n",
    "    \n",
    "    # Generate test data with correct dimensions\n",
    "    nx = 12  # State dimension (fixed for quadrotor)\n",
    "    nu = 4   # Control dimension (fixed for quadrotor)\n",
    "    x0_test, xref_test, uref_test = generate_test_data(nx, nu, N)\n",
    "    print(f\"Generated test data with dimensions: x0({nx}), xref({N},{nx}), uref({N-1},{nu})\")\n",
    "    \n",
    "    # Initialize hardware solver\n",
    "    print(f\"Loading bitstream: {bitstream_path}\")\n",
    "    hw_solver = tinympc_hw(bitstream_path=str(bitstream_path))\n",
    "    \n",
    "    # Test each max_iter value\n",
    "    for max_iter in maxiter_values:\n",
    "        print(f\"\\nTesting max_iter = {max_iter}\")\n",
    "        \n",
    "        # Set check_termination equal to max_iter as requested\n",
    "        check_termination = max_iter\n",
    "        hw_solver.setup(max_iter=max_iter, check_termination=check_termination, verbose=0)\n",
    "        \n",
    "        times = []\n",
    "        \n",
    "        # Run multiple trials\n",
    "        for trial in range(num_trials):\n",
    "            # Set problem data\n",
    "            hw_solver.set_x0(x0_test)\n",
    "            hw_solver.set_x_ref(xref_test)\n",
    "            hw_solver.set_u_ref(uref_test)\n",
    "            \n",
    "            # Measure execution time\n",
    "            start_time = time.perf_counter()\n",
    "            success = hw_solver.solve(timeout=1.0)\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            if success:\n",
    "                exec_time = (end_time - start_time) * 1000  # Convert to ms\n",
    "                times.append(exec_time)\n",
    "            else:\n",
    "                print(f\"  Trial {trial+1} failed\")\n",
    "        \n",
    "        if len(times) > 0:\n",
    "            results['maxiter'].append(max_iter)\n",
    "            results['mean_time'].append(np.mean(times))\n",
    "            results['std_time'].append(np.std(times))\n",
    "            results['min_time'].append(np.min(times))\n",
    "            results['max_time'].append(np.max(times))\n",
    "            results['all_times'].append(times)\n",
    "            \n",
    "            print(f\"  Mean time: {np.mean(times):.3f} ms (std: {np.std(times):.3f} ms)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    hw_solver.cleanup()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max_iter values to test\n",
    "maxiter_values = np.arange(10, 1000, 10)\n",
    "\n",
    "# Run performance tests\n",
    "if len(bitstreams) > 0:\n",
    "    print(f\"Testing bitstream: {selected_bitstream.name}\")\n",
    "    \n",
    "    # Extract N for this bitstream\n",
    "    N_selected = extract_N_from_bitstream(selected_bitstream)\n",
    "    print(f\"Bitstream N parameter: {N_selected}\")\n",
    "    print(f\"Max_iter values: {maxiter_values}\")\n",
    "    print(f\"Note: check_termination_iter = max_iter for all tests\\n\")\n",
    "    \n",
    "    results = test_performance_vs_maxiter(\n",
    "        selected_bitstream, \n",
    "        maxiter_values, \n",
    "        num_trials=10\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_results = pd.DataFrame({\n",
    "        'max_iter': results['maxiter'],\n",
    "        'mean_time_ms': results['mean_time'],\n",
    "        'std_time_ms': results['std_time'],\n",
    "        'min_time_ms': results['min_time'],\n",
    "        'max_time_ms': results['max_time']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"No bitstreams available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all available bitstreams\n",
    "all_results = {}\n",
    "\n",
    "if len(bitstreams) > 1:\n",
    "    print(f\"Testing all {len(bitstreams)} bitstreams...\\n\")\n",
    "    \n",
    "    for bitstream in bitstreams:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {bitstream.name}\")\n",
    "        \n",
    "        # Extract N for this bitstream\n",
    "        N_current = extract_N_from_bitstream(bitstream)\n",
    "        print(f\"Bitstream N parameter: {N_current}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            results = test_performance_vs_maxiter(\n",
    "                bitstream, \n",
    "                maxiter_values=np.arange(10, 500, 10),  # Reduced set for faster testing\n",
    "                num_trials=5\n",
    "            )\n",
    "            all_results[bitstream.name] = results\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to test {bitstream.name}: {e}\")\n",
    "    \n",
    "    # Compare results\n",
    "    if len(all_results) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for name, results in all_results.items():\n",
    "            if len(results['maxiter']) > 0:\n",
    "                plt.plot(results['maxiter'], results['mean_time'], \n",
    "                        'o-', label=name, markersize=8, linewidth=2)\n",
    "        \n",
    "        plt.xlabel('max_iter', fontsize=12)\n",
    "        plt.ylabel('Execution Time (ms)', fontsize=12)\n",
    "        plt.title('Performance Comparison Across Bitstreams', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Only one bitstream available, skipping comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(bitstreams) > 0 and len(results['maxiter']) > 0:\n",
    "    # Perform linear regression\n",
    "    x = np.array(results['maxiter'])\n",
    "    y = np.array(results['mean_time'])\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    \n",
    "    print(\"Linear Regression Results:\")\n",
    "    print(f\"  Slope: {slope:.6f} ms/iteration\")\n",
    "    print(f\"  Intercept: {intercept:.3f} ms\")\n",
    "    print(f\"  R-squared: {r_value**2:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"  Standard error: {std_err:.6f}\")\n",
    "    \n",
    "    # Generate fitted line\n",
    "    x_fit = np.linspace(0, max(x) * 1.1, 100)\n",
    "    y_fit = slope * x_fit + intercept\n",
    "    \n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  Each iteration adds approximately {slope:.6f} ms to execution time\")\n",
    "    print(f\"  Fixed overhead is approximately {intercept:.3f} ms\")\n",
    "    if r_value**2 > 0.95:\n",
    "        print(f\"  Excellent linear relationship (R² = {r_value**2:.4f})\")\n",
    "    elif r_value**2 > 0.90:\n",
    "        print(f\"  Strong linear relationship (R² = {r_value**2:.4f})\")\n",
    "    else:\n",
    "        print(f\"  Moderate linear relationship (R² = {r_value**2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(bitstreams) > 0 and len(results['maxiter']) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Execution time vs max_iter with error bars\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.errorbar(results['maxiter'], results['mean_time'], \n",
    "                 yerr=results['std_time'], \n",
    "                 fmt='o-', capsize=5, markersize=8, \n",
    "                 label='Measured', color='blue')\n",
    "    ax1.plot(x_fit, y_fit, 'r--', linewidth=2, \n",
    "             label=f'Linear fit: y = {slope:.4f}x + {intercept:.2f}')\n",
    "    ax1.set_xlabel('max_iter', fontsize=12)\n",
    "    ax1.set_ylabel('Execution Time (ms)', fontsize=12)\n",
    "    ax1.set_title('Execution Time vs max_iter', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.text(0.05, 0.95, f'R² = {r_value**2:.4f}', \n",
    "             transform=ax1.transAxes, fontsize=10, \n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Plot 2: Residuals from linear fit\n",
    "    ax2 = axes[0, 1]\n",
    "    residuals = y - (slope * x + intercept)\n",
    "    ax2.scatter(x, residuals, s=50, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "    ax2.set_xlabel('max_iter', fontsize=12)\n",
    "    ax2.set_ylabel('Residual (ms)', fontsize=12)\n",
    "    ax2.set_title('Residuals from Linear Fit', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Box plot of execution times\n",
    "    ax3 = axes[1, 0]\n",
    "    positions = results['maxiter']\n",
    "    widths = [p * 0.1 for p in positions]  # Variable width based on max_iter\n",
    "    bp = ax3.boxplot(results['all_times'], positions=positions, widths=widths,\n",
    "                      patch_artist=True, showmeans=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    ax3.set_xlabel('max_iter', fontsize=12)\n",
    "    ax3.set_ylabel('Execution Time (ms)', fontsize=12)\n",
    "    ax3.set_title('Distribution of Execution Times', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Performance metrics\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Create performance summary table\n",
    "    table_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Bitstream', selected_bitstream.name],\n",
    "        ['Linear Coefficient', f'{slope:.6f} ms/iter'],\n",
    "        ['Fixed Overhead', f'{intercept:.3f} ms'],\n",
    "        ['R-squared', f'{r_value**2:.4f}'],\n",
    "        ['Max tested iter', f'{max(results[\"maxiter\"])}'],\n",
    "        ['Min exec time', f'{min(results[\"min_time\"]):.3f} ms'],\n",
    "        ['Max exec time', f'{max(results[\"max_time\"]):.3f} ms']\n",
    "    ]\n",
    "    \n",
    "    table = ax4.table(cellText=table_data, \n",
    "                      colWidths=[0.5, 0.5],\n",
    "                      cellLoc='left',\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Style the header row\n",
    "    for i in range(2):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Style the data rows\n",
    "    for i in range(1, len(table_data)):\n",
    "        for j in range(2):\n",
    "            table[(i, j)].set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')\n",
    "    \n",
    "    ax4.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.suptitle(f'Hardware Performance Analysis: {selected_bitstream.name}', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig('hardware_performance_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nFigure saved as 'hardware_performance_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "if len(bitstreams) > 0 and 'df_results' in locals():\n",
    "    output_file = f\"performance_results_{selected_bitstream.stem}.csv\"\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    print(f\"Results exported to: {output_file}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE TEST SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Bitstream: {selected_bitstream.name}\")\n",
    "    print(f\"Linear relationship: Time(ms) = {slope:.6f} * max_iter + {intercept:.3f}\")\n",
    "    print(f\"R-squared: {r_value**2:.4f}\")\n",
    "    print(f\"Conclusion: {'Strong' if r_value**2 > 0.95 else 'Moderate'} linear relationship confirmed\")\n",
    "    print(f\"Per-iteration cost: {slope:.6f} ms\")\n",
    "    print(f\"Fixed overhead: {intercept:.3f} ms\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitstream switch time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "all_bitstreams = list(Path(\"bitstream\").glob(\"*.bit\"))\n",
    "\n",
    "# Test bitstream switch time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BITSTREAM SWITCH TIME TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(all_bitstreams) < 2:\n",
    "    print(\"Need at least 2 bitstreams to test switching time\")\n",
    "else:\n",
    "    # Initialize results storage\n",
    "    switch_times = []\n",
    "    \n",
    "    # Number of switches to test\n",
    "    num_tests = 10\n",
    "    \n",
    "    print(f\"Testing {num_tests} switches between bitstreams...\")\n",
    "    \n",
    "    # Alternate between first two bitstreams\n",
    "    for i in range(num_tests):\n",
    "        bitstream = all_bitstreams[i % len(all_bitstreams)]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        new_solver = tinympc_hw(bitstream_path=str(bitstream))\n",
    "        switch_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        switch_times.append(switch_time)\n",
    "        print(f\"Switch {i+1}: {switch_time:.2f} ms\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_switch = np.mean(switch_times)\n",
    "    std_switch = np.std(switch_times)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Average switch time: {avg_switch:.2f} ms\")\n",
    "    print(f\"Standard deviation: {std_switch:.2f} ms\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tests TinyMPC hardware performance by:\n",
    "1. Loading different bitstreams onto FPGA\n",
    "2. Measuring execution time for various max_iter values\n",
    "3. Setting check_termination_iter = max_iter for consistent testing\n",
    "4. Performing linear regression to verify the linear relationship\n",
    "5. Visualizing the results with comprehensive plots\n",
    "\n",
    "The results confirm that execution time has a linear relationship with max_iter, as expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
